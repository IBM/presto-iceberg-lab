{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#presto-workshop-building-an-open-data-lakehouse-with-presto","title":"Presto Workshop - Building an Open Data Lakehouse with Presto","text":"<p>Welcome to our workshop! In this workshop, you\u2019ll learn the basics of Presto, the open-source SQL query engine, and it's support for Iceberg. You\u2019ll get Presto running locally on your machine and connect to an S3-based data source and a REST server, which enables our Iceberg support. This is a beginner-level workshop for software developers and engineers who are new to Presto and Iceberg. At the end of the workshop, you will understand how to integrate Presto with Iceberg and MinIO and how to understand the Iceberg table format.</p> <p>The goals of this workshop are to show you:</p> <ul> <li>What is Apache Iceberg and how to use it</li> <li>How to connect Presto to MinIO s3 storage and an Iceberg-compatible REST server using Docker</li> <li>How to take advantage of Iceberg using Presto and why you would want to</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Agenda</li> <li>Compatibility</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Introduction Introduction to the technologies used Prerequisite Prerequisites for the workshop Lab 1: Set up an Open Lakehouse Set up a Presto cluster with REST catalog and data source connection Lab 2: Set up the Data Source Set up a storage bucket in MinIO Lab 3: Exploring Iceberg Tables Explore how to create Iceberg tables and use Iceberg features"},{"location":"#compatibility","title":"Compatibility","text":"<p>This workshop has been tested on the following platforms:</p> <ul> <li>Linux: Ubuntu 22.04</li> <li>MacOS: M1 Mac</li> </ul>"},{"location":"#technology-used","title":"Technology Used","text":"<ul> <li>Docker: A container engine to run several applications in self-contained containers.</li> <li>Presto: A fast and Reliable SQL Engine for Data Analytics and the Open Lakehouse</li> <li>Apache Iceberg: A high-performance format for huge analytic tables</li> <li>MinIO: A high-performance, S3 compatible object store</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>Kiersten Stokes</li> <li>Yihong Wang</li> </ul>"},{"location":"introduction/","title":"Introduction","text":"<p>A data lakehouse is a data platform that merges the best aspects of data warehouses and data lakes into one data management solution. If you are looking for an open-source solution for a data lakehouse, Presto is the perfect choice. Presto is a fast and reliable SQL query engine for data analytics on the open lakehouse. It has a wide variety of use cases, like running interactive/ad hoc queries at sub-second performance for your high-volume apps, or lengthy ETL jobs that aggregate or join terabytes of data. Presto is designed to be adaptive, flexible, and extensible. The plugin mechanism it provides allows you to connect to different data sources. A single Presto query can combine data from multiple sources, archiving analytics across your entire organization. Dozens of connectors are available from the Presto community today. Here is a high-level architecture diagram:</p> <p></p>"},{"location":"introduction/#data-lakehouses","title":"Data Lakehouses","text":"<p>In order to understand the data lakehouse, we should cover the other data storage solutions that it is based upon: the data warehouse and the data lake.</p> <ul> <li>Data warehouse: a storage system where relational (row- and column-based) tables are stored in order to perform fast analytics</li> <li>Data lake: a storage system where a great deal of semi-structured (XML, and JSON) or unstructured data (files, images, time-series, etc.) is stored as-is for a relatively low cost</li> </ul> <p>A data lakehouse combines the query speed and data quality of a data warehouse with the flexibility and low storage cost of a data lake. An open lakehouse has the additional benefit of being based on open technologies. A general diagram of the architecture of a data lakehouse is below.</p> <p></p> <p>As seen in the above diagram, Presto and Apache Iceberg are open source projects that each have a place in the open data lakehouse. Let's look at where they fit in the ecosystem below.</p>"},{"location":"introduction/#presto-overview","title":"Presto Overview","text":"<p>Presto is a query engine, which is a piece of software that sits on top of the underlying data storage architecture and fulfills requests for data by optimizing the data retrieval process. More specifically, Presto is a distributed query engine for fast SQL-based analytics.</p> <p>Presto is flexible and supports querying across diverse sources, including both structured relational databases and unstructured and semi-structured NoSQL data sources. Presto uses what it calls 'connectors' to integrate with this wide range of external data sources. Any data source can be queried as long as the data source adapts to the API expected by Presto. This makes Presto extremely flexible and extensible. Likewise, Presto supports many different file formats, such as ORC, Avro, Parquet, CSV, JSON, and more. Presto also supports a few different table formats including Delta Lake and Apache Iceberg.</p> <p>Presto was open-sourced in 2019 when it was donated to the Linux Foundation and is under the open source governance of the Presto Foundation. All of these reasons make it the perfect choice of query engine for an open data lakehouse.</p>"},{"location":"introduction/#table-formats","title":"Table Formats","text":"<p>In the above section, we mentioned that Presto supports multiple file formats and multiple table formats. Let's explore the meaning of these terms more closely. A file format is just the structure of a file that tells a program how to display its contents, as specified by the file extension. For example <code>.txt</code> is a file format. In the case of a data lakehouse, there are a few different file formats that can be used to store table data. Some popular options are Avro, Parquet, and Orc.</p> <p>A table format, on the other hand, is more like a metadata layer between the data files and whatever is trying to access the table that is represented by these files. A table format determines how the files that make up a table are organized and brings database-like features and efficiency to a data lake. In this way, it is a key part of a data lakehouse. Table formats support features that often aren't available on traditional data lakes, such as ACID transactions and row-level operations. The three most prevalent open table formats are Apache Iceberg, Apache Hudi, and Delta Lake. Take a look at the comparison table below:</p> <p></p>"},{"location":"introduction/#apache-iceberg","title":"Apache Iceberg","text":"<p>Apache Iceberg is a popular table format because it is fast, efficient, and reliable at any scale. Iceberg simplifies data processing on the large datasets stored in data lakehouses due to its unique set of metadata that it stores for each table. Here is a diagram from the Apache Iceberg documentation that explains this metadata:</p> <p></p> <p>As we can see, table state is maintained using a series of files. The table metadata file tracks the table schema, partitioning config, and custom table properties - in other words, a snapshot of the table. More technically, a snapshot represents the state of a table at some time and is what is used to access the complete set of data files in the table.</p> <p>The manifest list stores a list of the manifest files associated with the snapshot to which the manifest list applies, as well as some metadata about those manifest files. A manifest file points to a handful of data files that are associated with the manifest list, as well as additional metadata about those data files. Iceberg supports data files in Parquet, ORC, and Avro format.</p> <p>Changes made to a table are efficient because a new metadata file is created and replaces the old metadata with an atomic swap.</p>"},{"location":"introduction/#getting-started","title":"Getting Started","text":"<p>In this workshop, you will use docker to spin up</p> <ul> <li>A Presto cluster consisting of a single server node</li> <li>An Iceberg-compatible REST server backed by a PostgreSQL database</li> <li>A MinIO s3-compatible storage instance</li> </ul> <p>Without further ado, let's get started.</p> <p></p>"},{"location":"lab-1/","title":"Set up an Open Lakehouse","text":"<p>In this section, you will use docker compose to create an open lakehouse with the following components:</p> <ul> <li>A MinIO s3 Object Storage as the data storage component</li> <li>A REST server to keep track of table metadata</li> <li>A single-node Presto cluster as the SQL query engine</li> </ul> <p>This section is comprised of the following steps:</p> <ul> <li>Set up an Open Lakehouse</li> <li>1. Spin up all containers<ul> <li>Lakehouse architecture</li> <li>Iceberg catalogs</li> <li>Lookig at the Docker Compose file</li> </ul> </li> <li>2. Check that services have started</li> <li>3. Connect to Iceberg</li> </ul>"},{"location":"lab-1/#1-spin-up-all-containers","title":"1. Spin up all containers","text":"<p>Bring up the necessary containers with the following command:</p> <pre><code>docker compose up -d\n</code></pre> <p>This command may take quite awhile to run, as docker has to pull an image for each container and start them. While we wait for startup to complete, let's cover some important background architecture for our lakehouse and see how it relates to the <code>docker-compose.yml</code> file. This file defines everything about our multi-container application.</p>"},{"location":"lab-1/#lakehouse-architecture","title":"Lakehouse architecture","text":"<p>Recall that, at minimum, a lakehouse consists of a processing engine, data stored in a lake format such as s3-compatible storage, and a table format to organize stored data into tables. The first two pieces are somewhat straigtforward for this workshop: we spin up a Presto container for processing and a MinIO container for storage. The table format we will use is Iceberg - but where does Iceberg fit into the picture? We don't spin up an Iceberg container because Iceberg isn't an executable, it's simply a specification of how a table is organized. How, then, do we hook Iceberg into our lakehouse? This is where Iceberg catalogs come into play.</p>"},{"location":"lab-1/#iceberg-catalogs","title":"Iceberg catalogs","text":"<p>In order to use Iceberg with Presto, we have to set up an Iceberg catalog. Recall that Iceberg is a table format rather than a catalog itself. The Iceberg table format manages most of its metadata in metadata files in the underlying storage (in this case MinIO s3 object storage) alongside the raw data files. A small amount of metadata, however, still requires the use of a meta-datastore, which is what is referred to as an \"Iceberg catalog\". The Iceberg catalog is a central place to find the current names and locations of the current metadata pointer for each fully-qualified table in a particular data store. It is also responsible for making the \"atomic swap\" between different versions of metadata. Iceberg supports several different meta-datastore implementations, most of which Presto also supports. When using Presto, this storage can be provided by Hive, Nessie, Glue, Hadoop, or via a REST server implementation.  We are using a REST server implementation in this workshop.</p>"},{"location":"lab-1/#lookig-at-the-docker-compose-file","title":"Lookig at the Docker Compose file","text":"<p>First, we define a network: <code>presto_network</code>. Each of our containers will communicate across this network.</p> <p>The next section is the <code>service</code> section, which is the bulk of the file. The first service we define is that of the Presto cluster, which we have named <code>presto-coordinator</code>. We provide a human-readable <code>container_name</code> (also \"presto-coordinator\") and the Docker <code>image</code> that we want this service to be based on, which is the <code>presto</code> image with tag <code>latest</code> hosted in the <code>prestodb</code> DockerHub repository. The value <code>8080:8080</code> means that we want to map port 8080 on the Docker host (left side of the colon) to port 8080 in the container (right of the colon).</p> <p>We also need to supply the Presto container with some necessary configuration files, which we define using the <code>volume</code> key. Similar to how we defined the port, we're saying here that we want to map the files that are in the <code>presto/etc</code> directory (relative to our current working directory on the command line) to the location is the container corresponding to <code>/opt/presto-server/etc</code>, which is the directory that Presto expects to find configuration files. Here are the configuration settings for the Presto server as given in <code>./presto/etc/config.properties</code> that we will pass to our server container:</p> <pre><code>coordinator=true\nnode-scheduler.include-coordinator=true\nhttp-server.http.port=8080\ndiscovery-server.enabled=true\ndiscovery.uri=http://localhost:8080\nnode.environment=test\n</code></pre> <ul> <li><code>coordinator</code>: defines whether this Presto server acts as a coordinator or not. Use value <code>true</code> for a coordinator</li> <li><code>node-scheduler.include-coordinator</code>: defines whether the Presto server acts as a worker as well as a coordinator. We use the value <code>true</code> to accept worker tasks since we only have one node in our Presto cluster</li> <li><code>http-server.http.port</code>: defines the port number for the HTTP server</li> <li><code>discovery-server.enabled</code>: defines whether the Presto server should act as a discovery server to register workers</li> <li><code>discovery.uri</code>: defines the discovery server's URI, which is itself in this case</li> <li><code>node.environment</code>: defines the name of the environment; all Presto nodes in a cluster must have the same environment name</li> </ul> <p>Next we specify any necesssary environment variables. In this case, we give the username and password required to access our MinIO storage. Finally, we state that this container is part of the previouly-created <code>presto_network</code>, meaning it will be able to communicate with other services on the network.</p> <p>Now let's look at the <code>iceberg-rest</code> service more closely. This service uses an open source image originally created by Tabular that implements a simple REST server that adheres to the Iceberg REST specification. The code that makes up this image is a simple REST wrapper around an underlying JDBC Iceberg catalog, making it a \"proxy\" REST server implementation. Note that we set several environment variables inn the Docker compose file for this Iceberg REST service. Those beginning with <code>CATALOG_</code> are the catalog configuration variables used to create the Iceberg catalog when the image starts up. We are specifying here that we want to use s3 storage and that the catalog warehouse will reside at the endpoint <code>jdbc:postgresql://postgres:5432/metastore_db</code>. In this case, <code>postgres</code> in the endpoint refers to the <code>postgres</code> container <code>hostname</code> defined a few lines down in the YAML file. <code>metastore_db</code> is the name of the PostgreSQL database we want to use for storage, and is created when the <code>postgres</code> service starts. The last property that we will call out is <code>depends_on</code>, which defines dependencies between our service containers. In this case, the <code>postgres</code> container will be started before the <code>iceberg-rest</code> container. This makes sense since the PostgreSQL database needs to be running before the REST metastore service can start.</p> <p>You should now have the context you need in order to understand the configuration for the remaining <code>services</code>: <code>postgres</code> and <code>minio</code>. These services don't require as much setup as the others. On the last few lines of the file, we define additional <code>volumes</code>. These are different from those that we created on the fly in the <code>services</code> section in that here we create named volumes that can be persisted even if some containers need to restart.</p> <p>The output of the <code>up</code> command will look like the below when all containers have been started:</p> <pre><code>[+] Running 9/9\n \u2714 Network conf_presto-network   Created         0.0s\n \u2714 Volume \"conf_minio-data\"      Created         0.0s\n \u2714 Volume \"conf_postgres-data\"   Created         0.0s\n \u2714 Volume \"conf_catalog-data\"    Created         0.0s\n \u2714 Container presto-coordinator  Started         1.0s\n \u2714 Container postgres            Started         1.0s\n \u2714 Container minio               Started         1.0s\n \u2714 Container iceberg-rest        Started         1.4s\n \u2714 Container mc                  Started         1.3s\n</code></pre>"},{"location":"lab-1/#2-check-that-services-have-started","title":"2. Check that services have started","text":"<p>Let's also check that our relevant services have started.</p> <pre><code>docker logs --tail 100 minio\n</code></pre> <p>If started successfully, the logs for the <code>minio</code> container should include something similar to the below:</p> <pre><code>API: http://172.23.0.4:9090  http://127.0.0.1:9090 \nWebUI: http://172.23.0.4:9091 http://127.0.0.1:9091 \n</code></pre> <p>We will be using the console address in the next exercise. Let's check that the REST server is running with the following command:</p> <pre><code>docker logs --tail 50 iceberg-rest\n</code></pre> <p>If the REST service is up and running properly, you should see the below lines somewhere near the bottom of the logs, likely interspersed with other logging information.</p> <pre><code>...\n2024-11-04T19:57:01.800 INFO  [org.apache.iceberg.rest.RESTCatalogServer] - Creating catalog with properties: {jdbc.password=password, s3.endpoint=http://minio:9090, jdbc.user=user, io-impl=org.apache.iceberg.aws.s3.S3FileIO, catalog-impl=org.apache.iceberg.jdbc.JdbcCatalog, jdbc.schema-version=V1, warehouse=s3://warehouse/, uri=jdbc:postgresql://postgres:5432/metastore_db}\n...\n2024-11-04T19:57:02.213 INFO  [org.eclipse.jetty.server.AbstractConnector] - Started ServerConnector@1151e434{HTTP/1.1, (http/1.1)}{0.0.0.0:8181}\n2024-11-04T19:57:02.214 INFO  [org.eclipse.jetty.server.Server] - Started @539ms\n...\n</code></pre> <p>If the REST server is up, the PostgreSQL database also must be up because the metastore requires this on startup.</p> <p>Now, let's check the coordinator node:</p> <pre><code>docker logs --tail 100 presto-coordinator\n</code></pre> <p>If the Presto server is up and running properly, the last lines of the output would like the following:</p> <pre><code>2023-11-14T04:03:22.246Z        INFO    main    com.facebook.presto.storage.TempStorageManager  -- Loading temp storage local --\n2023-11-14T04:03:22.251Z        INFO    main    com.facebook.presto.storage.TempStorageManager  -- Loaded temp storage local --\n2023-11-14T04:03:22.256Z        INFO    main    com.facebook.presto.server.PrestoServer ======== SERVER STARTED ========\n</code></pre> <p>The Presto server will likely take the longest to start up. If you don't see any errors or the <code>SERVER STARTED</code> notice, wait a few minutes and check the logs again.</p> <p>You can also assess the status of your cluster using the Presto UI at the relevant IP address: <code>http://&lt;your_ip&gt;:8080</code>. If you're running everything on your local machine, the address will be <code>http://localhost:8080</code>. You should see 1 active worker (which is the coordinator node, in our case) and a green \"ready\" status in the top right corner, as seen below.</p> <p></p>"},{"location":"lab-1/#3-connect-to-iceberg","title":"3. Connect to Iceberg","text":"<p>Our containers are up and running, but you may be wondering how Presto works with Iceberg, as we didn't see any special key-value pairs for this in the docker compose file. Presto makes it very easy to get started with Iceberg, with no need to install any additional packages. If we started the Presto CLI right now, we would be able to create tables in Iceberg format - but how? Recall the volume that we passed to the <code>presto-coordinator</code> container. This volume includes a directory called <code>catalog</code> that was mapped to the <code>/opt/presto-server/etc/catalog</code> location in the container along with the other server configuration files. The <code>catalog</code> directory is where the Presto server looks to see what underlying data sources should be made available to Presto and how to connect to those sources. Let's take a look at the <code>iceberg.properties</code> file that was mapped to the Presto cluster.</p> <pre><code>connector.name=iceberg\niceberg.catalog.type=rest\niceberg.rest.uri=http://iceberg-rest:8181\niceberg.catalog.warehouse=s3://warehouse/\nhive.s3.path-style-access=true\nhive.s3.endpoint=http://minio:9090\nhive.s3.aws-access-key=minio\nhive.s3.aws-secret-key=minio123\n</code></pre> <p>This file includes a required <code>connector.name</code> property that indicates we're defining properties for an Iceberg connector. It also lists <code>rest</code> as the Iceberg catalog type, as we're using the REST catalog to support our Iceberg tables, and supplies the URI for the REST endpoint. The catalog warehouse is defined with the same value we supplied to the REST server when it started up. The remaining configuration options give the details needed in order to access our underlying s3 data source. When Presto starts, it accesses these configuration files in order to determine which connections it can make.</p> <p>Leveraging high-performance huge-data analytics is as easy as that! Let's move to the next exercise to set up our data source and start creating some Iceberg tables.</p> <p></p>"},{"location":"lab-2/","title":"Set up the Data Source","text":"<p>In this section, you will prepare the MinIO instance to use with Presto.</p> <p>This section is comprised of the following steps:</p> <ul> <li>Set up the Data Source</li> <li>1. Create a storage bucket</li> </ul>"},{"location":"lab-2/#1-create-a-storage-bucket","title":"1. Create a storage bucket","text":"<p>As we have already seen, MinIO will act as our underlying data storage for this workshop. MinIO is a high-performance, open-source object storage system. Because it implements the AWS s3 API (i.e., it is \"s3-compatible\"), we are able to use it to integrate with Presto. These types of object stores use buckets to organize files, giving us a specific location to provide to Presto when we read and write tables.</p> <p>As part of our setup, a bucket called <code>warehouse</code> has already been created in our MinIO storage. See the <code>mc</code> service <code>entrypoint</code> in the <code>docker-compose.yaml</code> to for details on how this bucket has been created. Let's verify this by accessing the MinIO UI in a browser at the relevant IP address: <code>http://&lt;your_ip&gt;:8443</code>. If you're running everything on your local machine, the address will be <code>http://localhost:8443</code>.</p> <p>Note</p> <p>If you are using a remote Linux host, use the command <code>curl https://ipinfo.io/ip</code> to get your public IP address.</p> <p>Note</p> <p>Recall that we assigned a port mapping of <code>8443:9091</code> in <code>docker-compose.yml</code>, meaning that within the Docker network, the MinIO UI exists on port 9091, but it is exposed on port 8443 when accessing it from outside of the container.</p> <p>You will be prompted for a username and password, which are <code>minio</code> and <code>minio123</code> respectively, once again as defined in our <code>docker-compose.yml</code> for the <code>minio</code> service <code>environment</code> variables. Once you are logged in, you should see that the bucket titled <code>warehouse</code> is present.</p> <p>If you see a webpage like the below that indicates that there are no available buckets, you can create one now. Click \"Create a Bucket\".</p> <p></p> <p>Enter the name <code>warehouse</code> and create the bucket. The name of the bucket must be <code>warehouse</code> in this case because that is the value that we passed to the REST server on container start for the catalog warehouse (<code>CATALOG_WAREHOUSE=s3://warehouse/</code>).</p> <p>That's it! You can view the empty bucket in the \"Object brower\". Now our s3 object store is ready for use. Let's move to the next section to start creating Iceberg tables in Presto.</p> <p>Note</p> <p>If you run into errors in the third section of this workshop, there may have been more errors in your docker setup than just a missing bucket. Running <code>docker restart minio</code>, then <code>docker restart mc</code> a few moments later should resolve this issue.</p> <p></p>"},{"location":"lab-3/","title":"Exploring Iceberg Tables","text":"<p>In this section, we will create Iceberg tables and explore their structure in the underlying data source. We will also take a look at the hidden tables that Presto provides for Iceberg that give information on the table metadata. We'll also use some of Iceberg's key features - schema evolution and time travel - from Presto to get an idea of how they work.</p> <p>This section is comprised of the following steps:</p> <ul> <li>Exploring Iceberg Tables</li> <li>1. Creating a schema</li> <li>2. Creating an Iceberg table</li> <li>3. Iceberg schema evolution</li> <li>4. Iceberg time travel</li> </ul>"},{"location":"lab-3/#1-creating-a-schema","title":"1. Creating a schema","text":"<p>First, let's learn how to run the Presto CLI to connect to the coordinator. There are several ways to do that:</p> <ol> <li>Download the executable jar from the official repository and run the jar file with a proper JVM. You can see details in this documentation.</li> <li>Use the <code>presto-cli</code> that comes with the <code>prestodb/presto</code> Docker image</li> </ol> <p>For this lab, since we run everything on Docker containers, we are going to use the second approach. You can run the <code>presto-cli</code> inside the coordinator container with the below command:</p> <pre><code>$ docker exec -it presto-coordinator presto-cli\npresto&gt;\n</code></pre> <p>Note</p> <p>Since the <code>presto-cli</code> is executed inside the <code>coordinator</code> and <code>localhost:8080</code> is the default server, there is no need to specify the <code>--server</code> argument.</p> <p>After you run the command, the prompt should change from the shell prompt <code>$</code> to the <code>presto&gt;</code> CLI prompt. Run the SQL statement <code>show catalogs</code> to see a list of currently configured catalogs:</p> <pre><code>presto&gt; show catalogs;\n Catalog\n---------\n hive\n iceberg\n jmx\n memory\n system\n tpcds\n tpch\n(7 rows)\n\nQuery 20231122_230131_00021_79xda, FINISHED, 1 node\nSplits: 19 total, 19 done (100.00%)\n[Latency: client-side: 173ms, server-side: 163ms] [0 rows, 0B] [0 rows/s, 0B/s]\n</code></pre> <p>These are the catalogs that we specified when launching the coordinator container by using the configurations from the <code>presto/catalog</code> directory. The <code>hive</code> and <code>iceberg</code> catalogs here are expected, but here is a short description of the rest:</p> <ul> <li>jmx: The JMX connector provides the ability to query JMX   information from all nodes in a Presto cluster.</li> <li>memory: The Memory connector stores all data and metadata   in RAM on workers and both are discarded when Presto restarts.</li> <li>system: The System connector provides information and   metrics about the currently running Presto cluster.</li> <li>tpcds: The TPCDS connector provides a set of schemas   to support the TPC Benchmark\u2122 DS (TPC-DS)</li> <li>tpch: The TPCH connector provides a set of schemas to   support the TPC Benchmark\u2122 H (TPC-H).</li> </ul> <p>Now, let's create a schema. A schema is a logical way to organize tables within a catalog. We'll create a schema called \"minio\" within our \"iceberg\" catalog. We also want to specify that the tables within this schema are all located in our s3 storage, and more specifically, in the <code>warehouse</code> bucket that we created previously.</p> <pre><code>presto&gt; CREATE SCHEMA iceberg.minio with (location = 's3a://warehouse/');\nCREATE SCHEMA\n</code></pre> <p>We'll be working almost exclusively with the \"iceberg\" catalog and \"minio\" schema, so we can employ a <code>USE</code> statement to indicate that all the queries we run will be against tables in this catalog/schema combination unless specificed. Otherwise, we would have to use the fully-qualified table name for every statement (i.e., <code>iceberg.minio.&lt;table_name&gt;</code>).</p> <pre><code>presto&gt; USE iceberg.minio;\nUSE\npresto:minio&gt;\n</code></pre> <p>You'll notice that the prompt has changed to also include the schema we're working in. Now we're ready to create a table!</p>"},{"location":"lab-3/#2-creating-an-iceberg-table","title":"2. Creating an Iceberg table","text":"<p>When creating a new table, we specify the name and the table schema. A table schema is different than the schema we've been referring to up until now. The table schema defines the column names and types. Let's create a table to represent the books that a (very small) library has in their inventory.</p> <pre><code>presto:minio&gt; CREATE TABLE books (id bigint, title varchar, author varchar) WITH (location = 's3a://warehouse/minio/books');\nCREATE TABLE\n</code></pre> <p>We now have the table structure, but no data. Let's look into this a little bit. Pull up your MinIO UI and navigate to the <code>warehouse/minio/books</code> directory. Notice that the <code>minio</code> and <code>books</code> directories were created implicitly with the location property that we passed to <code>CREATE TABLE</code>. There should be a single folder at this location called <code>metadata</code>. If you go into this directory, you'll see a single metadata file with the extension <code>.metadata.json</code>, which stores the table schema information.</p> <p>Let's add some data to this table:</p> <pre><code>presto:minio&gt; INSERT INTO books VALUES (1, 'Pride and Prejudice', 'Jane Austen'), (2, 'To Kill a Mockingbird', 'Harper Lee'), (3, 'The Great Gatsby', 'F. Scott Fitzgerald');\nINSERT: 3 rows\n\nQuery 20231123_021811_00005_79xda, FINISHED, 1 node\nSplits: 35 total, 35 done (100.00%)\n[Latency: client-side: 0:01, server-side: 0:01] [0 rows, 0B] [0 rows/s, 0B/s]\n</code></pre> <p>We can verify our data by running a <code>SELECT *</code> statement:</p> <pre><code>presto:minio&gt; SELECT * FROM books;\n id |         title         |       author\n----+-----------------------+---------------------\n  1 | Pride and Prejudice   | Jane Austen\n  2 | To Kill a Mockingbird | Harper Lee\n  3 | The Great Gatsby      | F. Scott Fitzgerald\n(3 rows)\n</code></pre> <p>If we go back to our MinIO UI now, we can see a new folder, <code>data</code> in the <code>warehouse/minio/books</code> path. The <code>data</code> folder has a single <code>.parquet</code> data file inside. This structure of <code>data</code> and <code>metadata</code> folders is the default for Iceberg tables.</p> <p>We can query some of the Iceberg metadata information from Presto. Let's look at the hidden \"history\" table from Presto. Note that the quotation marks are required here.</p> <pre><code>presto:minio&gt; SELECT * FROM \"books$history\";\n       made_current_at       |     snapshot_id     | parent_id | is_current_ancestor\n-----------------------------+---------------------+-----------+---------------------\n 2023-12-04 03:22:51.654 UTC | 7120201811871583704 | NULL      | true\n(1 row)\n\nQuery 20231204_032649_00007_8ds9i, FINISHED, 1 node\nSplits: 17 total, 17 done (100.00%)\n[Latency: client-side: 0:04, server-side: 0:04] [1 rows, 17B] [0 rows/s, 4B/s]\n</code></pre> <p>This shows us that we have a snapshot that was created at the moment we inserted data. We can get more details about the snapshot with the below query:</p> <pre><code>presto:minio&gt; SELECT * FROM \"books$snapshots\";\n        committed_at         |     snapshot_id     | parent_id | operation |                                                manifest_list                                                |                                                                                                           summary\n-----------------------------+---------------------+-----------+-----------+-------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n 2023-12-04 03:22:51.654 UTC | 7120201811871583704 | NULL      | append    | s3a://warehouse/minio/books/metadata/snap-7120201811871583704-1-c736f70c-53b0-46bd-93e5-5df38eb0ef62.avro | {changed-partition-count=1, added-data-files=1, total-equality-deletes=0, added-records=3, total-position-deletes=0, added-files-size=579, total-delete-files=0, total-files-size=579, total-records=3, total-data-files=1}\n(1 row)\n</code></pre> <p>This gets us a little more information, such as the type of operation, the manifest list file that this snapshot refers to, as well as a summary of the changes that were made as a result of this operation.</p> <p>Let's go one level deeper and look at the current manifest list metadata:</p> <pre><code>presto:minio&gt; SELECT * FROM \"books$manifests\";\n                                        path                                         | length | partition_spec_id |  added_snapshot_id  | added_data_files_count | existing_data_files_count | deleted_data_files_count | partitions\n-------------------------------------------------------------------------------------+--------+-------------------+---------------------+------------------------+---------------------------+--------------------------+------------\n s3a://warehouse/minio/books/metadata/c736f70c-53b0-46bd-93e5-5df38eb0ef62-m0.avro |   6783 |                 0 | 7120201811871583704 |                      1 |                         0 |                        0 | []\n(1 row)\n</code></pre> <p>As promised, the manifest list table show us a list of the manifest files (or file, in this case) associated with our current state.</p> <p>Lastly, let's look at what the manifests can tell us. To do so, we call on the <code>files</code> hidden table:</p> <pre><code>presto:minio&gt; SELECT * FROM \"books$files\";\n content |                                    file_path                                    | file_format | record_count | file_size_in_bytes |     column_sizes     |  value_counts   | null_value_counts | nan_value_counts |                 lower_bounds                  |               upper_bounds               | key_metadata | split_offsets | equality_ids\n---------+---------------------------------------------------------------------------------+-------------+--------------+--------------------+----------------------+-----------------+-------------------+------------------+-----------------------------------------------+------------------------------------------+--------------+---------------+--------------\n       0 | s3a://warehouse/minio/books/data/27b61673-a995-4810-9aa5-b4675b8483ce.parquet | PARQUET     |            3 |                579 | {1=52, 2=124, 3=103} | {1=3, 2=3, 3=3} | {1=0, 2=0, 3=0}   | {}               | {1=1, 2=Pride and Prejud, 3=F. Scott Fitzger} | {1=3, 2=To Kill a Mockio, 3=Jane Austen} | NULL         | NULL          | NULL\n(1 row)\n</code></pre> <p>We have here a path to the data file and some metadata for that file that can help when determining which files need to be accessed for a certain query.</p> <p>There are other hidden tables as well that you can interrogate. Here is a summary of all hidden tables that Presto can provide:</p> <ul> <li><code>$properties</code>: General properties of the given table</li> <li><code>$history</code>: History of table state changes</li> <li><code>$snapshots</code>: Details about the table snapshots</li> <li><code>$manifests</code>: Details about the manifest lists of different table snapshots</li> <li><code>$partitions</code>: Detailed partition information for the table</li> <li><code>$files</code>: Overview of data files in the current snapshot of the table</li> </ul>"},{"location":"lab-3/#3-iceberg-schema-evolution","title":"3. Iceberg schema evolution","text":"<p>The Iceberg connector also supports in-place table evolution, aka schema evolution, such as adding, dropping, and renaming columns. This is one of Iceberg's key features. Let's try it. Let's say we want to add a column to indicate whether a book has been checked out. We'll run the following command to do so:</p> <pre><code>presto:minio&gt; ALTER TABLE books ADD COLUMN checked_out boolean;\nADD COLUMN\n</code></pre> <p>At this point, a new <code>.metadata.json</code> file is created and can be viewed in the MinIO UI, but, once again, no updates to the other metadata files or the hidden tables take place until data is added. The library comes into the possession of a new book and it is immediately checked out. We can add data for that:</p> <pre><code>presto:minio&gt; INSERT INTO books VALUES (4, 'One Hundred Years of Solitude', 'Gabriel Garcia Marquez', true);\nINSERT: 1 row\n\nQuery 20231123_025430_00013_79xda, FINISHED, 1 node\nSplits: 35 total, 35 done (100.00%)\n[Latency: client-side: 0:01, server-side: 0:01] [0 rows, 0B] [0 rows/s, 0B/s]\n</code></pre> <p>At this point, a new snapshot is made current, which we can see by querying the hidden snapshot table:</p> <pre><code>presto:minio&gt; SELECT * FROM \"books$snapshots\";\n        committed_at         |     snapshot_id     |      parent_id      | operation |                                                manifest_list                                                |                                                                                                           summary\n-----------------------------+---------------------+---------------------+-----------+-------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n 2023-12-04 03:22:51.654 UTC | 7120201811871583704 | NULL                | append    | s3a://warehouse/minio/books/metadata/snap-7120201811871583704-1-c736f70c-53b0-46bd-93e5-5df38eb0ef62.avro | {changed-partition-count=1, added-data-files=1, total-equality-deletes=0, added-records=3, total-position-deletes=0, added-files-size=579, total-delete-files=0, total-files-size=579, total-records=3, total-data-files=1}\n 2023-12-04 03:33:37.630 UTC | 5122816232892408908 | 7120201811871583704 | append    | s3a://warehouse/minio/books/metadata/snap-5122816232892408908-1-973a8dc3-8103-4df7-8324-1fa13a2f1202.avro | {changed-partition-count=1, added-data-files=1, total-equality-deletes=0, added-records=1, total-position-deletes=0, added-files-size=765, total-delete-files=0, total-files-size=1344, total-records=4, total-data-files=2}\n(2 rows)\n</code></pre> <p>The output confirms that we now have a new snapshot, and a new manifest list file representing it.</p>"},{"location":"lab-3/#4-iceberg-time-travel","title":"4. Iceberg time travel","text":"<p>Another popular feature of Iceberg is time travel, wherein we can query the table state from a given time or snapshot ID. It's also possible to rollback the state of a table to a previous snapshot using its ID. For the purposes of our example, let's say the person that checked out One Hundred Days of Solitude enjoyed it so much that they bought it from the library, taking it out of the inventory. We want to roll the table state back to before we inserted the latest row. Let's first get our snapshot IDs.</p> <pre><code>presto:minio&gt; SELECT snapshot_id, committed_at FROM \"books$snapshots\" ORDER BY committed_at;\n     snapshot_id     |        committed_at\n---------------------+-----------------------------\n 7120201811871583704 | 2023-12-04 03:22:51.654 UTC\n 5122816232892408908 | 2023-12-04 03:33:37.630 UTC\n(2 rows)\n</code></pre> <p>Let's verify that the table is in the expected state at our earliest snapshot ID:</p> <pre><code>presto:minio&gt; SELECT * FROM books FOR VERSION AS OF 7120201811871583704;\n id |         title         |       author        | checked_out\n----+-----------------------+---------------------+-------------\n  1 | Pride and Prejudice   | Jane Austen         | NULL\n  2 | To Kill a Mockingbird | Harper Lee          | NULL\n  3 | The Great Gatsby      | F. Scott Fitzgerald | NULL\n(3 rows)\n</code></pre> <p>We could also do the same thing using a timestamp or date. If you run this query, make sure you change the timestamp so that it's accurate for the time at which you're following along.</p> <pre><code>presto:minio&gt; SELECT * FROM books FOR TIMESTAMP AS OF TIMESTAMP '2023-12-04 03:22:51.700 UTC';\n id |         title         |       author        | checked_out\n----+-----------------------+---------------------+-------------\n  1 | Pride and Prejudice   | Jane Austen         | NULL\n  2 | To Kill a Mockingbird | Harper Lee          | NULL\n  3 | The Great Gatsby      | F. Scott Fitzgerald | NULL\n(3 rows)\n</code></pre> <p>Now that we've verified the table state that we want to roll back to, we can call a procedure on the \"iceberg\" catalog's built-in <code>system</code> schema to do so:</p> <pre><code>presto:minio&gt; CALL iceberg.system.rollback_to_snapshot('minio', 'books', 7120201811871583704);\nCALL\n</code></pre> <p>Let's verify that the table is back to how it was before:</p> <pre><code>presto:minio&gt; SELECT * FROM books;\n id |         title         |       author        | checked_out\n----+-----------------------+---------------------+-------------\n  1 | Pride and Prejudice   | Jane Austen         | NULL\n  2 | To Kill a Mockingbird | Harper Lee          | NULL\n  3 | The Great Gatsby      | F. Scott Fitzgerald | NULL\n(3 rows)\n</code></pre> <p>Notice that the table still includes the <code>checked_out</code> column. This is to be expected because the snapshot only changes when data files are written to. Removing the column would be another schema evolution operation that only changes the <code>.metadata.json</code> file and not the snapshot itself.</p> <p>You just explored some of Iceberg's key features using Presto! Presto's Iceberg connector has more features than those we've gone over today, such as partitioning and partition column transforms, as well as additional features that are soon-to-come!</p> <p></p>"},{"location":"prerequisite/","title":"Prerequisite","text":"<p>This workshop uses the Docker and Docker Compose CLI tools to set up a Presto cluster, a local REST server on top of a PostgreSQL database, and a MinIO s3 object storage instance. We recommend Podman, which is a rootless - and hence more secure - drop-in replacement for Docker. Install Podman and ensure that <code>podman</code> has been successfully <code>alias</code>'ed to <code>docker</code> in your working environment.</p>"},{"location":"prerequisite/#clone-the-workshop-repository","title":"Clone the workshop repository","text":"<p>Various parts of this workshop will require the configuration files from the workshop repository. Use the following command to download the whole repository:</p> <pre><code>git clone https://github.com/IBM/presto-iceberg-lab.git\ncd presto-iceberg-lab\n</code></pre> <p>Alternatively, you can download the repository as a zip file, unzip it and change into the <code>presto-iceberg-lab</code> main directory.</p>"},{"location":"resources/CONTRIBUTORS/","title":"Contributors","text":""},{"location":"resources/CONTRIBUTORS/#remko-de-knikker","title":"Remko de Knikker","text":"<ul> <li>Github: remkohdev</li> <li>Twitter: @remkohdev</li> <li>LinkedIn: remkohdev</li> <li>Medium: @remkohdev</li> </ul>"},{"location":"resources/CONTRIBUTORS/#steve-martinelli","title":"Steve Martinelli","text":"<ul> <li>Github: stevemar</li> <li>Twitter: @stevebot</li> <li>LinkedIn: stevemar</li> </ul>"},{"location":"resources/MKDOCS/","title":"mkdocs examples","text":"<p>This page includes a few neat tricks that you can do with <code>mkdocs</code>. For a complete list of examples visit the mkdocs documentation.</p>"},{"location":"resources/MKDOCS/#code","title":"Code","text":"<pre><code>print(\"hello world!\")\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-line-numbers","title":"Code with line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-highlights","title":"Code with highlights","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-tabs","title":"Code with tabs","text":"Tab Header <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> Another Tab Header <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"resources/MKDOCS/#more-tabs","title":"More tabs","text":"Windows <p>If on windows download the <code>Win32.zip</code> file and install it.</p> MacOS <p>Run <code>brew install foo</code>.</p> Linux <p>Run <code>apt-get install foo</code>.</p>"},{"location":"resources/MKDOCS/#checklists","title":"Checklists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> In hac habitasse platea dictumst</li> </ul>"},{"location":"resources/MKDOCS/#add-a-button","title":"Add a button","text":"<p>Launch the lab</p> <p>Visit IBM Developer</p> <p>Sign up! </p>"},{"location":"resources/MKDOCS/#call-outs","title":"Call outs","text":"<p>Tip</p> <p>You can use <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code> <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>quote</code> or <code>example</code>.</p> <p>Note</p> <p>A note.</p> <p>Abstract</p> <p>An abstract.</p> <p>Info</p> <p>Some info.</p> <p>Success</p> <p>A success.</p> <p>Question</p> <p>A question.</p> <p>Warning</p> <p>A warning.</p> <p>Danger</p> <p>A danger.</p> <p>Example</p> <p>A example.</p> <p>Bug</p> <p>A bug.</p>"},{"location":"resources/MKDOCS/#call-outs-with-code","title":"Call outs with code","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.</p>"},{"location":"resources/MKDOCS/#formatting","title":"Formatting","text":"<p>In addition to the usual italics, and bold there is now support for:</p> <ul> <li>highlighted</li> <li>underlined</li> <li>strike-through</li> </ul>"},{"location":"resources/MKDOCS/#tables","title":"Tables","text":"OS or Application Username Password Windows VM <code>Administrator</code> <code>foo</code> Linux VM <code>root</code> <code>bar</code>"},{"location":"resources/MKDOCS/#emojis","title":"Emojis","text":"<p>Yes, these work.  </p>"},{"location":"resources/MKDOCS/#images","title":"Images","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/MKDOCS/#right-align-image","title":"right align image","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/RESOURCES/","title":"Additional resources","text":""},{"location":"resources/RESOURCES/#data-lakehouse","title":"Data Lakehouse","text":"<ul> <li>What is a Data Lakehouse</li> <li>A Gentle Introduction to Data Lakehouse</li> </ul>"},{"location":"resources/RESOURCES/#presto","title":"Presto","text":"<ul> <li>Presto</li> <li>Presto Documentation</li> </ul>"},{"location":"resources/RESOURCES/#iceberg","title":"Iceberg","text":"<ul> <li>Iceberg</li> <li>Iceberg Documentation</li> <li>Iceberg Table Spec</li> <li>Understanding Iceberg Table Metadata</li> </ul>"},{"location":"resources/RESOURCES/#minio","title":"MinIO","text":"<ul> <li>MinIO</li> <li>MinIO Documentation</li> </ul>"}]}